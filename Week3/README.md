# ğŸ§  Anime Data Analytics using Apache Spark

## ğŸ“Œ Introduction
This project uses **Apache Spark** to process and analyze data from two main sources:
- `animes.csv`: information about anime series.
- `ratings.csv`: user rating data.

**Objectives:**
1. Calculate the average and total number of ratings for each anime.
2. Analyze the number of ratings of different anime types by **user** and by **year**.
3. Compute the average rating of **users by genre**.
4. Compare the performance among **CSV**, **Parquet**, and **ORC** formats.
5. Save the results to **HDFS** with partitions by `year/month/day/hour`.

---

## ğŸ—‚ï¸ Directory Structure
```
Week3/
â”‚
â”œâ”€â”€ data/ # Source data
â”‚ â”œâ”€â”€ animes.csv
â”‚ â”œâ”€â”€ ratings.csv
â”‚ â””â”€â”€ id_to_genreids.json
â”‚
â”œâ”€â”€ Spark/
â”‚ â”œâ”€â”€ preprocess_data.ipynb # Main Spark data processing notebook
â”‚ â””â”€â”€ save_hdfs.ipynb # Notebook to save data to HDFS
â”‚
â”œâ”€â”€ output/
â”‚ â”œâ”€â”€ csv/
â”‚ â”‚ â””â”€â”€ animes_best_rated/
â”‚ â”œâ”€â”€ evaluate/
â”‚ â”‚ â”œâ”€â”€ csv/
â”‚ â”‚ â”œâ”€â”€ orc/
â”‚ â”‚ â””â”€â”€ parquet/
â”‚ â””â”€â”€ parquet/
â”‚ â”œâ”€â”€ animes_best_rated/
â”‚ â”œâ”€â”€ genres_user/
â”‚ â””â”€â”€ type_per_year/
â”‚
â””â”€â”€ README.md
```



## âš™ï¸ 1ï¸âƒ£ Analysis #1 â€” Average and Total Ratings per Anime

### ğŸ“˜ Description
- Read `ratings.csv` and `animes.csv`.
- Join both datasets by `anime_id`.
- Calculate:
  - `avg_rating` â€“ average score of each anime.
  - `total_votes` â€“ total number of ratings.

### ğŸ§® Spark Code Logic
```python
from pyspark.sql.functions import avg, count

ratings = spark.read.csv("data/ratings.csv", header=True, inferSchema=True)
animes = spark.read.csv("data/animes.csv", header=True, inferSchema=True)

animes_best_rated = (
    ratings.groupBy("anime_id")
    .agg(
        avg("rating").alias("avg_rating"),
        count("rating").alias("total_votes")
    )
    .join(animes, "anime_id")
)
```
ğŸ“¤ Output

Dataset: animes_best_rated

Output Path: /output/parquet/animes_best_rated/

Fields: anime_id, name, avg_rating, total_votes

## âš™ï¸ 2ï¸âƒ£ Analysis #2 â€” Number of Ratings per Anime Type by User per Year
### ğŸ“˜ Description

Combine rating data with anime genre/type information.

Extract year from the timestamp column.

Count the number of ratings by user, genre, and year.

### ğŸ§® Spark Code Logic
```
from pyspark.sql.functions import year, explode, col, count

genres_user = (
    ratings.withColumn("year", year("timestamp"))
    .join(animes.select("anime_id", "genres"), "anime_id")
    .withColumn("genre", explode(col("genres")))
    .groupBy("user_id", "year", "genre")
    .agg(count("*").alias("total_rated"))
)
```
### ğŸ“¤ Output

Dataset: genres_user

Output Path: /output/parquet/genres_user/

Fields: user_id, year, genre, total_rated

## âš™ï¸ 3ï¸âƒ£ Analysis #3 â€” Average User Rating by Genre
### ğŸ“˜ Description

Each user may watch multiple genres â†’ explode the genres column.

Calculate the average rating per user for each genre.

### ğŸ§® Spark Code Logic
```
from pyspark.sql.functions import avg, explode, col

type_per_year = (
    ratings.join(animes, "anime_id")
    .withColumn("genre", explode(col("genres")))
    .groupBy("user_id", "genre")
    .agg(avg("rating").alias("avg_rating"))
)
```
### ğŸ“¤ Output

Dataset: type_per_year

Output Path: /output/parquet/type_per_year/

Fields: user_id, genre, avg_rating

## ğŸ§¾ 4ï¸âƒ£ Format Comparison: CSV vs Parquet vs ORC

### ğŸ§ª Evaluation

The same dataset (animes_best_rated) is read and measured by:

File size

Read/write speed

Query performance (filter, groupBy)

Format	Read Speed	  File Size 	Schema Support	 Write Speed
CSV	    âŒ Slow	    ğŸ”º Large	  âŒ No	          âš ï¸ Slow
Parquet	âœ… Fast	    âœ… Small	     âœ… Yes	         âœ… Very Fast
ORC	    âœ… Very Fast	âœ… Smallest	 âœ… Yes	         âœ… Fast
â¡ï¸ Conclusion

Parquet is the most efficient format for large-scale data analytics in Spark due to its columnar storage and compression capabilities.

## ğŸ—ƒï¸ 5ï¸âƒ£ Saving Data to HDFS

### ğŸ§± HDFS Directory Structure
```
/user/hdfs/week4/
â”œâ”€â”€ animes_best_rated/
â”œâ”€â”€ genres_user/
â””â”€â”€ type_per_year/
```
### ğŸ“˜ Time-based Partitioning

When saving to HDFS, data is partitioned by:

year

month

day

hour

### ğŸ§® Example Code
```
animes_best_rated.write.mode("overwrite") \
    .partitionBy("year", "month", "day", "hour") \
    .parquet("hdfs://namenode:9000/user/hdfs/week4/animes_best_rated")

Same for Other Datasets
genres_user.write.mode("overwrite") \
    .partitionBy("year", "month", "day", "hour") \
    .parquet("hdfs://namenode:9000/user/hdfs/week4/genres_user")

type_per_year.write.mode("overwrite") \
    .partitionBy("year", "month", "day", "hour") \
    .parquet("hdfs://namenode:9000/user/hdfs/week4/type_per_year")
```
### ğŸ“ˆ Summary Table
```
Analysis	Objective	Dataset Output	HDFS Path
#1	Average & total ratings per anime	animes_best_rated	/week4/animes_best_rated
#2	Ratings by user & genre per year	genres_user	/week4/genres_user
#3	Average rating per user per genre	type_per_year	/week4/type_per_year
```
### ğŸ’¡ Future Enhancements

Automate the pipeline using Spark Submit + Airflow

Add visualizations in Power BI or Tableau

Perform sentiment analysis on anime descriptions

### ğŸ‘¨â€ğŸ’» Author

Hoang Minh Hai
ğŸ“… Project: Week 3 â€“ Anime Data Analysis with Apache Spark
ğŸ§© Environment: Python 3.12, PySpark 4.0.1, HDFS, Parquet/ORC